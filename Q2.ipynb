{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqaNg22dIQYk"
      },
      "source": [
        "## **Data Load**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import all library\n",
        "\n",
        "import scipy.io as sio\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.decomposition import IncrementalPCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import timeit\n",
        "import tracemalloc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kvKXTR5vIKod",
        "outputId": "292750b3-4f4b-40bc-dc47-739c24820059"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training set shape: (2576, 416)\n",
            "Test set shape: (2576, 104)\n"
          ]
        }
      ],
      "source": [
        "mat_content = sio.loadmat('assets/face.mat')\n",
        "# mat_content # Let's see the content...\n",
        "\n",
        "face_data = mat_content['X']\n",
        "face_labels = mat_content['l']\n",
        "\n",
        "x_train, x_test, y_train, y_test = [], [], [], []\n",
        "\n",
        "n_classes = 52\n",
        "images_per_class = 10\n",
        "\n",
        "for i in range(n_classes):\n",
        "    start_idx = i * images_per_class\n",
        "    end_idx = start_idx + images_per_class\n",
        "\n",
        "    x_train.append(face_data[:, start_idx:start_idx+8])\n",
        "    x_test.append(face_data[:, start_idx+8:end_idx])\n",
        "\n",
        "    y_train.append(face_labels[:, start_idx:start_idx+8])\n",
        "    y_test.append(face_labels[:, start_idx+8:end_idx])\n",
        "\n",
        "x_train, x_test = np.hstack(x_train), np.hstack(x_test)\n",
        "y_train, y_test = np.hstack(y_train), np.hstack(y_test)\n",
        "\n",
        "x_train = x_train.astype(np.int64)\n",
        "x_test = x_test.astype(np.int64)\n",
        "\n",
        "# Output the shapes of the training and testing sets\n",
        "print(f\"Training set shape: {x_train.shape}\")\n",
        "print(f\"Test set shape: {x_test.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esnkRq0-IV_R"
      },
      "source": [
        "## **Q2. Incremental PCA**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKqn9RwZIqXy"
      },
      "source": [
        "creating subdatasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "oPbhJMvHIOul"
      },
      "outputs": [],
      "source": [
        "x_sub1, x_sub2, x_sub3, x_sub4 = [], [], [], []\n",
        "y_sub1, y_sub2, y_sub3, y_sub4 = [], [], [], []\n",
        "\n",
        "n_classes = 52\n",
        "images_per_class = 8\n",
        "\n",
        "for i in range(n_classes):\n",
        "  x_sub1.append(x_train[:,i*images_per_class :i*images_per_class + 2])\n",
        "  x_sub2.append(x_train[:,i*images_per_class+2 :i*images_per_class + 4])\n",
        "  x_sub3.append(x_train[:,i*images_per_class+4 :i*images_per_class + 6])\n",
        "  x_sub4.append(x_train[:,i*images_per_class+6 :i*images_per_class + 8])\n",
        "\n",
        "  y_sub1.append(y_train[:,i*images_per_class :i*images_per_class + 2])\n",
        "  y_sub2.append(y_train[:,i*images_per_class+2 :i*images_per_class + 4])\n",
        "  y_sub3.append(y_train[:,i*images_per_class+4 :i*images_per_class + 6])\n",
        "  y_sub4.append(y_train[:,i*images_per_class+6 :i*images_per_class + 8])\n",
        "\n",
        "x_sub1, x_sub2, x_sub3, x_sub4 = np.hstack(x_sub1), np.hstack(x_sub2), np.hstack(x_sub3), np.hstack(x_sub4)\n",
        "y_sub1, y_sub2, y_sub3, y_sub4 = np.hstack(y_sub1), np.hstack(y_sub2), np.hstack(y_sub3), np.hstack(y_sub4)\n",
        "\n",
        "x_subs = [x_sub1, x_sub2, x_sub3, x_sub4]\n",
        "y_subs = [y_sub1, y_sub2, y_sub3, y_sub4]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oxMM0mQJnkW"
      },
      "source": [
        "1. Incremental learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current memory usage: 0.035434 MB\n",
            "Peak memory usage: 11.59286 MB\n",
            "Execution Time: 0.13093221 seconds\n",
            "Accuracy: 38.46%\n"
          ]
        }
      ],
      "source": [
        "def run_ipca():\n",
        "  ipca = IncrementalPCA(n_components = 5)\n",
        "  \n",
        "  for subset in x_subs:\n",
        "    ipca.partial_fit(subset.T)\n",
        "  \n",
        "  return ipca \n",
        "\n",
        "def ipca_nn(ipca):\n",
        "  # Transform training subsets\n",
        "  W_train_sub1 = ipca.transform(x_sub1.T).T\n",
        "  W_train_sub2 = ipca.transform(x_sub2.T).T\n",
        "  W_train_sub3 = ipca.transform(x_sub3.T).T\n",
        "  W_train_sub4 = ipca.transform(x_sub4.T).T\n",
        "\n",
        "  # Optionally combine transformed training data if needed\n",
        "  W_train = np.vstack((W_train_sub1.T, W_train_sub2.T, W_train_sub3.T, W_train_sub4.T))\n",
        "\n",
        "  # Fit a classifier (e.g., k-NN) on the transformed training data\n",
        "  nn = KNeighborsClassifier(n_neighbors=1, metric='manhattan')\n",
        "  # You may need to combine y_train corresponding to each subset for training\n",
        "  y_train_combined = np.concatenate([y_sub1.T, y_sub2.T, y_sub3.T, y_sub4.T])  # Combine true labels\n",
        "\n",
        "  nn.fit(W_train, y_train_combined.ravel())\n",
        "\n",
        "  # Transform the test data\n",
        "  W_test = ipca.transform(x_test.T).T\n",
        "\n",
        "  # Predict using the classifier\n",
        "  y_pred = nn.predict(W_test.T)\n",
        "\n",
        "  # Calculate accuracy\n",
        "  accuracy = accuracy_score(y_test.T, y_pred)\n",
        "  print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "\n",
        "tracemalloc.start() \n",
        "start_time = timeit.default_timer()  # Start timing\n",
        "\n",
        "run_ipca()\n",
        "\n",
        "current, peak = tracemalloc.get_traced_memory()\n",
        "tracemalloc.stop()\n",
        "end_time = timeit.default_timer()  # Stop timing\n",
        "\n",
        "print(f\"Current memory usage: {current / 10**6} MB\")\n",
        "print(f\"Peak memory usage: {peak / 10**6} MB\")\n",
        "print(f\"Execution Time: {end_time - start_time:.8f} seconds\")\n",
        "ipca = run_ipca()\n",
        "ipca_nn(ipca)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkNdXRFUJsDW"
      },
      "source": [
        "2. Batch PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LNcYD3auI9Bk",
        "outputId": "9fdaea74-4337-47ee-866e-6ca381b2d18d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current memory usage: 0.016898 MB\n",
            "Peak memory usage: 9.616852 MB\n",
            "Execution Time: 0.03387150 seconds\n",
            "Accuracy: 39.42%\n"
          ]
        }
      ],
      "source": [
        "def batch_pca_nn():\n",
        "  # Step 1: Get PCA  result from training data & project all training data\n",
        "  batch_pca = PCA(n_components = 5)\n",
        "  batch_pca.fit(x_train.T)\n",
        "  return batch_pca\n",
        "\n",
        "def batch_nn(batch_pca):\n",
        "  W_train = batch_pca.transform(x_train.T).T\n",
        "\n",
        "  # Step 2: Project test data\n",
        "  W_test = batch_pca.transform(x_test.T).T\n",
        "\n",
        "  # Step 3: Get result using NN classifier\n",
        "  nn = KNeighborsClassifier(n_neighbors=1, metric='manhattan')\n",
        "  nn.fit(W_train.T, y_train.reshape(-1))\n",
        "  y_pred = nn.predict(W_test.T)\n",
        "\n",
        "  # Step 4: Calculate accuracy\n",
        "  accuracy = accuracy_score(y_test.T, y_pred)\n",
        "  print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "\n",
        "tracemalloc.start() \n",
        "start_time = timeit.default_timer()  # Start timing\n",
        "\n",
        "batch_pca_nn()\n",
        "\n",
        "current, peak = tracemalloc.get_traced_memory()\n",
        "tracemalloc.stop()\n",
        "end_time = timeit.default_timer()  # Stop timing\n",
        "\n",
        "print(f\"Current memory usage: {current / 10**6} MB\")\n",
        "print(f\"Peak memory usage: {peak / 10**6} MB\")\n",
        "print(f\"Execution Time: {end_time - start_time:.8f} seconds\")\n",
        "\n",
        "batch_pca = batch_pca_nn()\n",
        "batch_nn(batch_pca)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFSD8yl-MOLq"
      },
      "source": [
        "3. train with only first subset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MRVNb2bKMNjm",
        "outputId": "24559753-df5c-4c93-f2a9-fef3042b951d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current memory usage: 0.003042 MB\n",
            "Peak memory usage: 3.136444 MB\n",
            "Execution Time: 0.01315092 seconds\n",
            "Accuracy: 17.31%\n"
          ]
        }
      ],
      "source": [
        "def sub1_pca_nn():\n",
        "  # Step 1: Get PCA  result from training data & project all training data\n",
        "  sub1_pca = PCA(n_components = 5)\n",
        "  sub1_pca.fit(x_sub1.T)\n",
        "  return sub1_pca\n",
        "\n",
        "def sub1_nn(sub1_pca):\n",
        "  W_train = sub1_pca.transform(x_sub1.T).T\n",
        "\n",
        "  # Step 2: Project test data\n",
        "  W_test = sub1_pca.transform(x_test.T).T\n",
        "\n",
        "  # Step 3: Get result using NN classifier\n",
        "  nn = KNeighborsClassifier(n_neighbors=1, metric='manhattan')\n",
        "  nn.fit(W_train.T, y_sub1.reshape(-1))\n",
        "  y_pred = nn.predict(W_test.T)\n",
        "\n",
        "  # Step 4: Calculate accuracy\n",
        "  accuracy = accuracy_score(y_test.T, y_pred)\n",
        "  print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "\n",
        "tracemalloc.start() \n",
        "start_time = timeit.default_timer()  # Start timing\n",
        "\n",
        "sub1_pca_nn()\n",
        "\n",
        "current, peak = tracemalloc.get_traced_memory()\n",
        "tracemalloc.stop()\n",
        "end_time = timeit.default_timer()  # Stop timing\n",
        "\n",
        "print(f\"Current memory usage: {current / 10**6} MB\")\n",
        "print(f\"Peak memory usage: {peak / 10**6} MB\")\n",
        "print(f\"Execution Time: {end_time - start_time:.8f} seconds\")\n",
        "\n",
        "sub1_pca = sub1_pca_nn()\n",
        "sub1_nn(sub1_pca)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
